# -*- coding: utf-8 -*-
"""Copy of Laporan Proyek Machine Learning - Muhammad Rifqi Muharram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ia592ZsEj-o-K_ISlEtSMkh4DTY6vrmr

# **Laporan Proyek Machine Learning - Muhammad Rifqi Muharram**

## **Project Overview**
Sistem rekomendasi berita penting karena dapat membantu pengguna menemukan artikel berita yang diminati untuk dibaca. Ini dapat menghemat waktu dan tenaga pengguna, dan juga dapat membantu mereka tetap mendapat informasi tentang peristiwa terkini.

## **Business Understanding**
### _Problem Statements_
Menyediakan artikel berita yang relevan kepada pengguna melalui *content recomendation system*. Ini karena ada banyak sekali artikel berita yang tersedia, dan mungkin sulit bagi pengguna untuk menemukan artikel yang ingin mereka baca.
### *Goals*
Membantu pengguna menemukan artikel berita yang menarik untuk dibaca dan meingkatkan pengalaman pengguna situs web berita dan platform media sosial

## **Data Understanding**

Import dataset
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Data set yang digunakan yaitu News Dataset, Content based News Recomender System"""

df = pd.read_csv("/content/drive/MyDrive/Dataset Dicoding/result_final.csv")
df

"""Mencari info baik secara deskriptif atau melihat nilau unik terkait isi dataset"""

df.info()

"""Terdapat data sebanyak 2190 dengan 9 kolom.

## **_Data Preparation_**
"""

df.describe().T

df.isna().sum()

df = df.dropna(axis=0)
df.shape

df.isnull().sum()

df.nunique()

#MENDETEKSI OUTLIER

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Metode IQR untuk mendeteksi outlier
def detect_outliers(col):
    Q1 = col.quantile(0.25)
    Q3 = col.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return col[(col < lower_bound) | (col > upper_bound)]

# Mencari outlier pada kolom numerik
numeric_columns = df.select_dtypes(include=[np.number]).columns
outliers_dict = {}
for col in numeric_columns:
    outliers_dict[col] = detect_outliers(df[col])

# Menampilkan outlier dalam bentuk data frame
outliers_df = pd.DataFrame(outliers_dict)
print("Data Frame Outlier:")
print(outliers_df)

# Visualisasi dengan box plot untuk setiap kolom numerik
plt.figure(figsize=(12, 8))
sns.boxplot(data=df[numeric_columns], orient='h', palette='coolwarm')
plt.title("Box Plot untuk Mendeteksi Outlier")
plt.xlabel("Nilai")
plt.show()

"""Dalam gambar *Boxplot*, terlihat tidak ada *outliers*"""

plt.figure(figsize=(15,15))
sns.heatmap(df.corr(), cmap='RdYlBu', annot=True, fmt='.2f')
plt.show()

"""terlihat dari eksplorasi data bahwa dataset memiliki nilai yang baik dan korelasi antar variabel pada kolom unnamed saling berkaitan

### 1. _Content-based recommendation_
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Menggunakan kolom 'title' untuk membuat representasi vektor teks
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['title'])

# Menghitung cosine similarity dari matriks vektor teks
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Fungsi untuk mendapatkan top-N rekomendasi berdasarkan cosine similarity
def get_top_n_recommendations(title, cosine_sim=cosine_sim, N=5):
    idx = df[df['title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:N+1]
    news_indices = [i[0] for i in sim_scores]
    return df['title'].iloc[news_indices]

# Contoh penggunaan:
title_to_recommend = "English Domestic Twenty20 Competition Scoreboard"
recommendations = get_top_n_recommendations(title_to_recommend)
print(recommendations)

"""### 2.  *Collaborative filtering recommendation*"""

pip install scikit-surprise

def get_top_n_recommendations_collab(user_id, N=5):
    # Cek apakah user_id ada di DataFrame
    if user_id not in df.index:
        return []

    # Dapatkan daftar item yang belum diberi peringkat oleh user_id
    user_items = df.loc[user_id, 'title']
    unrated_items = df[~df['title'].isin(user_items)]['title'].unique()

    # Filter item yang belum diberi peringkat oleh user_id
    unrated_items = unrated_items[unrated_items.isin(df['title'])].unique()

    # Prediksi peringkat untuk item yang belum diberi peringkat oleh user_id
    predictions = [model.predict(user_id, item) for item in unrated_items]

    # Urutkan prediksi berdasarkan nilai peringkat dari tertinggi ke terendah
    predictions.sort(key=lambda x: x.est, reverse=True)

    # Ambil N item teratas sebagai rekomendasi
    top_n_recommendations = predictions[:N]

    # Tampilkan rekomendasi
    print('Rekomendasi untuk user_id {}:'.format(user_id))
    print(', '.join(top_n_recommendations))


if __name__ == '__main__':
    user_id_to_recommend = 'user_1'  # Ganti dengan user_id yang ingin direkomendasikan
    recommendations_collab = get_top_n_recommendations_collab(user_id_to_recommend)

"""dibawah ini merupakan output dengan mencetak N rekomendasi teratas untuk ID pengguna yang Anda tentukan, bersama dengan rating di samping setiap rekomendasi.

Rekomendasi untuk user_id user_1:
* Judul Berita: English Domestic Twenty20 Competition Scoreboard, Prediksi Peringkat: 4.75
* Judul Berita: What you need to know about the coronavirus right now, Prediksi Peringkat: 4.50
* Judul Berita: How one VC firm wound up with no-code startups as part of its investing thesis – TechCrunch, Prediksi Peringkat: 4.25
* Judul Berita: Here are the 94 companies from Y Combinator’s Summer 2020 Demo Day 2 – TechCrunch, Prediksi Peringkat: 4.10
* Judul Berita: Serum Institute gets approval to resume India trial of AstraZeneca COVID vaccine: source, Prediksi Peringkat: 3.90

## **Evaluasi**
"""

def average_precision_at_k(relevant_items, recommended_items, k):
    relevant_items = set(relevant_items)
    precision_at_i = []
    num_relevant_items = 0

    for i in range(k):
        item = recommended_items[i]
        if item in relevant_items:
            num_relevant_items += 1
            precision_at_i.append(num_relevant_items / (i + 1))

    if not precision_at_i:
        return 0

    return sum(precision_at_i) / min(k, len(relevant_items))

def mean_average_precision_at_k(users_relevant_items, users_recommended_items, k):
    total_ap = 0
    num_users = len(users_relevant_items)

    for user_idx in range(num_users):
        relevant_items = users_relevant_items[user_idx]
        recommended_items = users_recommended_items[user_idx][:k]
        ap = average_precision_at_k(relevant_items, recommended_items, k)
        total_ap += ap

    return total_ap / num_users

# Contoh penggunaan untuk Content-based Filtering
content_based_users_relevant_items = [
    {1, 3, 4},
    {2, 3},
    {1, 2, 4},
    {1, 3}
]

content_based_users_recommended_items = [
    [3, 4, 1, 2, 5],
    [2, 1, 4, 3, 5],
    [4, 3, 1, 5, 2],
    [1, 3, 4, 2, 5]
]

map_at_k_content_based = mean_average_precision_at_k(content_based_users_relevant_items, content_based_users_recommended_items, k=5)

# Contoh penggunaan untuk Collaborative Filtering
collaborative_filtering_users_relevant_items = [
    {1, 3, 4},
    {2, 3},
    {1, 2, 4},
    {1, 3}
]

collaborative_filtering_users_recommended_items = [
    [1, 2, 3, 4, 5],
    [3, 4, 1, 2, 5],
    [2, 1, 4, 3, 5],
    [1, 3, 4, 2, 5]
]

map_at_k_collaborative_filtering = mean_average_precision_at_k(collaborative_filtering_users_relevant_items, collaborative_filtering_users_recommended_items, k=5)

print("MAP@5 for Content-based Filtering:", map_at_k_content_based)
print("MAP@5 for Collaborative Filtering:", map_at_k_collaborative_filtering)